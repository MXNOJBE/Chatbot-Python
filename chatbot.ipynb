{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b0a7c5-349f-480f-8efc-27a0c21cbaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement onednn-cpu-gomp (from versions: none)\n",
      "ERROR: No matching distribution found for onednn-cpu-gomp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import nltk\n",
    "import numpy\n",
    "import requests\n",
    "from nltk.stem import LancasterStemmer\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.models import model_from_yaml\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943257b-5dc8-4066-9ba6-7f97ae4b5cab",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 66\u001b[0m\n\u001b[0;32m     65\u001b[0m yaml_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m---> 66\u001b[0m myChatModel \u001b[38;5;241m=\u001b[39m model_from_yaml(loaded_model_yaml)\n\u001b[0;32m     67\u001b[0m myChatModel\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchatbotmodel.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\manoj\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\keras\\saving\\model_config.py:71\u001b[0m, in \u001b[0;36mmodel_from_yaml\u001b[1;34m(yaml_string, custom_objects)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parses a yaml model configuration file and returns a model instance.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03mNote: Since TF 2.6, this method is no longer supported and will raise a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    RuntimeError: announces that the method poses a security risk\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMethod `model_from_yaml()` has been removed due to security risk of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marbitrary code execution. Please use `Model.to_json()` and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`model_from_json()` instead.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     75\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Method `model_from_yaml()` has been removed due to security risk of arbitrary code execution. Please use `Model.to_json()` and `model_from_json()` instead.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m myChatModel\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m myChatModel\u001b[38;5;241m.\u001b[39mfit(training, output, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# serialize model to yaml and save it to disk\u001b[39;00m\n\u001b[0;32m     83\u001b[0m model_yaml \u001b[38;5;241m=\u001b[39m myChatModel\u001b[38;5;241m.\u001b[39mto_json()\n",
      "File \u001b[1;32mc:\\Users\\manoj\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1137\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1131\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m cluster_coordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1132\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy)\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), \\\n\u001b[0;32m   1135\u001b[0m      training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1136\u001b[0m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1138\u001b[0m       x\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m   1139\u001b[0m       y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   1140\u001b[0m       sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1141\u001b[0m       batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1142\u001b[0m       steps_per_epoch\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[0;32m   1143\u001b[0m       initial_epoch\u001b[38;5;241m=\u001b[39minitial_epoch,\n\u001b[0;32m   1144\u001b[0m       epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m   1145\u001b[0m       shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m   1146\u001b[0m       class_weight\u001b[38;5;241m=\u001b[39mclass_weight,\n\u001b[0;32m   1147\u001b[0m       max_queue_size\u001b[38;5;241m=\u001b[39mmax_queue_size,\n\u001b[0;32m   1148\u001b[0m       workers\u001b[38;5;241m=\u001b[39mworkers,\n\u001b[0;32m   1149\u001b[0m       use_multiprocessing\u001b[38;5;241m=\u001b[39muse_multiprocessing,\n\u001b[0;32m   1150\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1151\u001b[0m       steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution)\n\u001b[0;32m   1153\u001b[0m   \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1154\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\manoj\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1397\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1396\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manoj\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1151\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1148\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[0;32m   1149\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m-> 1151\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m adapter_cls(\n\u001b[0;32m   1153\u001b[0m     x,\n\u001b[0;32m   1154\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1163\u001b[0m     distribution_strategy\u001b[38;5;241m=\u001b[39mdistribute_lib\u001b[38;5;241m.\u001b[39mget_strategy(),\n\u001b[0;32m   1164\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   1166\u001b[0m strategy \u001b[38;5;241m=\u001b[39m distribute_lib\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[1;32mc:\\Users\\manoj\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:987\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_data_adapter\u001b[39m(x, y):\n\u001b[0;32m    986\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 987\u001b[0m   adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m    988\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    993\u001b[0m             _type_name(x), _type_name(y)))\n",
      "File \u001b[1;32mc:\\Users\\manoj\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:987\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_data_adapter\u001b[39m(x, y):\n\u001b[0;32m    986\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 987\u001b[0m   adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m    988\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    993\u001b[0m             _type_name(x), _type_name(y)))\n",
      "File \u001b[1;32mc:\\Users\\manoj\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:706\u001b[0m, in \u001b[0;36mDatasetAdapter.can_handle\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcan_handle\u001b[39m(x, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(x, (data_types\u001b[38;5;241m.\u001b[39mDatasetV1, data_types\u001b[38;5;241m.\u001b[39mDatasetV2)) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m--> 706\u001b[0m           _is_distributed_dataset(x))\n",
      "File \u001b[1;32mc:\\Users\\manoj\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1696\u001b[0m, in \u001b[0;36m_is_distributed_dataset\u001b[1;34m(ds)\u001b[0m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_distributed_dataset\u001b[39m(ds):\n\u001b[1;32m-> 1696\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ds, input_lib\u001b[38;5;241m.\u001b[39mDistributedDatasetInterface)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "with open(\"backend/intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "try:\n",
    "    with open(\"chatbot.pickle\", \"rb\") as file:\n",
    "        words, labels, training, output = pickle.load(file)\n",
    "\n",
    "except:\n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    output_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "\n",
    "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = output_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "    training = numpy.array(training)\n",
    "    output = numpy.array(output)\n",
    "\n",
    "    with open(\"chatbot.pickle\", \"wb\") as file:\n",
    "        pickle.dump((words, labels, training, output), file)\n",
    "\n",
    "try:\n",
    "    yaml_file = open('chatbotmodel.yaml', 'r')\n",
    "    loaded_model_yaml = yaml_file.read()\n",
    "    yaml_file.close()\n",
    "    myChatModel = model_from_yaml(loaded_model_yaml)\n",
    "    myChatModel.load_weights(\"chatbotmodel.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "except:\n",
    "    # Make our neural network\n",
    "    myChatModel = Sequential()\n",
    "    myChatModel.add(Dense(8, input_shape=[len(words)], activation='relu'))\n",
    "    myChatModel.add(Dense(len(labels), activation='softmax'))\n",
    "\n",
    "    # optimize the model\n",
    "    myChatModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # train the model\n",
    "    myChatModel.fit(training, output, epochs=1000, batch_size=8)\n",
    "\n",
    "    # serialize model to yaml and save it to disk\n",
    "    model_yaml = myChatModel.to_json()\n",
    "    with open(\"chatbotmodel.yaml\", \"w\") as y_file:\n",
    "        y_file.write(model_yaml)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    myChatModel.save_weights(\"chatbotmodel.h5\")\n",
    "    print(\"Saved model from disk\")\n",
    "\n",
    "\n",
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "\n",
    "    return numpy.array(bag)\n",
    "\n",
    "\n",
    "def chatWithBot(inputText):\n",
    "    currentText = bag_of_words(inputText, words)\n",
    "    currentTextArray = [currentText]\n",
    "    numpyCurrentText = numpy.array(currentTextArray)\n",
    "\n",
    "    if numpy.all((numpyCurrentText == 0)):\n",
    "        return \"I didn't get that, try again\"\n",
    "\n",
    "    result = myChatModel.predict(numpyCurrentText[0:1])\n",
    "    result_index = numpy.argmax(result)\n",
    "    tag = labels[result_index]\n",
    "\n",
    "    if result[0][result_index] > 0.7:\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                responses = tg['responses']\n",
    "\n",
    "        return random.choice(responses)\n",
    "\n",
    "    else:\n",
    "        return \"I didn't get that, try again\"\n",
    "\n",
    "\n",
    "def chat():\n",
    "    print(\"Start talking with the chatbot (try quit to stop)\")\n",
    "\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        print(chatWithBot(inp))\n",
    "\n",
    "\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98093913-7eaf-4134-a77d-1fbb6e41d473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
